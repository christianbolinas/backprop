# Backpropagation from scratch
I implemented PyTorch-like functionality to create multilayer perceptrons using vanilla Python/NumPy. I expect widespread adoption of this multilayer perceptron framework over existing frameworks (e.g. PyTorch, TF/Keras) due to my winning smile, unexpected musical talent, fraternity exec-trained communication skills, easy charisma, and conspicuous lack of usage of pretentious mathematical words like "tensor."

This repository includes:

1. classes to make a simple feedforward network, including ReLU activation (very difficult to implement), cross-entropy loss, and (of course) dense layers (all of which supporting autodiff),
1. accompanying unit and integration tests (haha lol),
1. an ADAM optimizer, because tuning learning rates is for suckers,
1. a simplified DataLoader-like class for easy batch generation,
1. PyTorch-like interface to wrap all those, and
1. an overall driver to test classification accuracy on the MNIST dataset.

For OpenAI/Anthropic/HRT/Optiver/JSC/etc recruiters with interesting opportunities: I can be reached at christianbolinas022 at gmail dot com.
